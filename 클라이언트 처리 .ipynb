{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module dask.dataframe.io.csv:\n",
      "\n",
      "read_csv(urlpath, blocksize=64000000, collection=True, lineterminator=None, compression=None, sample=256000, enforce=False, assume_missing=False, storage_options=None, include_path_column=False, **kwargs)\n",
      "    Read CSV files into a Dask.DataFrame\n",
      "    \n",
      "    This parallelizes the :func:`pandas.read_csv` function in the following ways:\n",
      "    \n",
      "    - It supports loading many files at once using globstrings:\n",
      "    \n",
      "        >>> df = dd.read_csv('myfiles.*.csv')  # doctest: +SKIP\n",
      "    \n",
      "    - In some cases it can break up large files:\n",
      "    \n",
      "        >>> df = dd.read_csv('largefile.csv', blocksize=25e6)  # 25MB chunks  # doctest: +SKIP\n",
      "    \n",
      "    - It can read CSV files from external resources (e.g. S3, HDFS) by\n",
      "      providing a URL:\n",
      "    \n",
      "        >>> df = dd.read_csv('s3://bucket/myfiles.*.csv')  # doctest: +SKIP\n",
      "        >>> df = dd.read_csv('hdfs:///myfiles.*.csv')  # doctest: +SKIP\n",
      "        >>> df = dd.read_csv('hdfs://namenode.example.com/myfiles.*.csv')  # doctest: +SKIP\n",
      "    \n",
      "    Internally ``dd.read_csv`` uses :func:`pandas.read_csv` and supports many of the\n",
      "    same keyword arguments with the same performance guarantees. See the docstring\n",
      "    for :func:`pandas.read_csv` for more information on available keyword arguments.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    urlpath : string or list\n",
      "        Absolute or relative filepath(s). Prefix with a protocol like ``s3://``\n",
      "        to read from alternative filesystems. To read from multiple files you\n",
      "        can pass a globstring or a list of paths, with the caveat that they\n",
      "        must all have the same protocol.\n",
      "    blocksize : str, int or None, optional\n",
      "        Number of bytes by which to cut up larger files. Default value is\n",
      "        computed based on available physical memory and the number of cores.\n",
      "        If ``None``, use a single block for each file.\n",
      "        Can be a number like 64000000 or a string like \"64MB\"\n",
      "    collection : boolean, optional\n",
      "        Return a dask.dataframe if True or list of dask.delayed objects if False\n",
      "    sample : int, optional\n",
      "        Number of bytes to use when determining dtypes\n",
      "    assume_missing : bool, optional\n",
      "        If True, all integer columns that aren't specified in ``dtype`` are assumed\n",
      "        to contain missing values, and are converted to floats. Default is False.\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc.\n",
      "    include_path_column : bool or str, optional\n",
      "        Whether or not to include the path to each particular file. If True a new\n",
      "        column is added to the dataframe called ``path``. If str, sets new column\n",
      "        name. Default is False.\n",
      "    **kwargs\n",
      "        Extra keyword arguments to forward to :func:`pandas.read_csv`.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Dask dataframe tries to infer the ``dtype`` of each column by reading a sample\n",
      "    from the start of the file (or of the first file if it's a glob). Usually this\n",
      "    works fine, but if the ``dtype`` is different later in the file (or in other\n",
      "    files) this can cause issues. For example, if all the rows in the sample had\n",
      "    integer dtypes, but later on there was a ``NaN``, then this would error at\n",
      "    compute time. To fix this, you have a few options:\n",
      "    \n",
      "    - Provide explicit dtypes for the offending columns using the ``dtype``\n",
      "      keyword. This is the recommended solution.\n",
      "    \n",
      "    - Use the ``assume_missing`` keyword to assume that all columns inferred as\n",
      "      integers contain missing values, and convert them to floats.\n",
      "    \n",
      "    - Increase the size of the sample using the ``sample`` keyword.\n",
      "    \n",
      "    It should also be noted that this function may fail if a CSV file\n",
      "    includes quoted strings that contain the line terminator. To get around this\n",
      "    you can specify ``blocksize=None`` to not split files into multiple partitions,\n",
      "    at the cost of reduced parallelism.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform == 'win32' : \n",
    "    df = dd.read_csv(r\"D:\\data\\LANL-Earthquake-Prediction/train.csv\")\n",
    "else :\n",
    "    df = dd.read_csv(\"../../data/earthquake/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoustic_data</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=150</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 450 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                acoustic_data time_to_failure\n",
       "npartitions=150                              \n",
       "                        int64         float64\n",
       "                          ...             ...\n",
       "...                       ...             ...\n",
       "                          ...             ...\n",
       "                          ...             ...\n",
       "Dask Name: from-delayed, 450 tasks"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클라이언트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:56033\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:56033' processes=4 cores=8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "        return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(x):\n",
    "        return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df.time_to_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df.acoustic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method GCDiagnosis._gc_callback of <distributed.utils_perf.GCDiagnosis object at 0x32796d048>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/utils_perf.py\", line 184, in _gc_callback\n",
      "    def _gc_callback(self, phase, info):\n",
      "KeyboardInterrupt\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 29193 was killed by signal 15\n",
      "distributed.nanny - WARNING - Worker process 29196 was killed by signal 15\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 29197 was killed by signal 15\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 29199 was killed by signal 15\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.utils - ERROR - 'tcp://127.0.0.1:56518'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 1500, in add_worker\n",
      "    yield self.handle_worker(comm=comm, worker=address)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 2412, in handle_worker\n",
      "    worker_comm = self.stream_comms[worker]\n",
      "KeyError: 'tcp://127.0.0.1:56518'\n",
      "distributed.utils - ERROR - 'tcp://127.0.0.1:56517'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/utils.py\", line 713, in log_errors\n",
      "    yield\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 1500, in add_worker\n",
      "    yield self.handle_worker(comm=comm, worker=address)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 2412, in handle_worker\n",
      "    worker_comm = self.stream_comms[worker]\n",
      "KeyError: 'tcp://127.0.0.1:56517'\n",
      "distributed.core - ERROR - 'tcp://127.0.0.1:56518'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/core.py\", line 412, in handle_comm\n",
      "    result = yield result\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 736, in run\n",
      "    yielded = self.gen.throw(*exc_info)  # type: ignore\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 1500, in add_worker\n",
      "    yield self.handle_worker(comm=comm, worker=address)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 2412, in handle_worker\n",
      "    worker_comm = self.stream_comms[worker]\n",
      "KeyError: 'tcp://127.0.0.1:56518'\n",
      "distributed.core - ERROR - 'tcp://127.0.0.1:56517'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/core.py\", line 412, in handle_comm\n",
      "    result = yield result\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 736, in run\n",
      "    yielded = self.gen.throw(*exc_info)  # type: ignore\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 1500, in add_worker\n",
      "    yield self.handle_worker(comm=comm, worker=address)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\n",
      "    value = future.result()\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/dahlmoon/anaconda/lib/python3.7/site-packages/distributed/scheduler.py\", line 2412, in handle_worker\n",
      "    worker_comm = self.stream_comms[worker]\n",
      "KeyError: 'tcp://127.0.0.1:56517'\n"
     ]
    }
   ],
   "source": [
    "A = client.map(square, df_s.compute())\n",
    "B = client.map(neg, df_a.compute())\n",
    "total = client.submit(sum, B)\n",
    "total.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
